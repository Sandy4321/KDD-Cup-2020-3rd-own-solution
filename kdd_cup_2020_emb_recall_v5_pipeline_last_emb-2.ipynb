{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path = '/root/kdd_cup_2020'\n",
    "train_path = path + '/underexpose_train/local_2'  \n",
    "test_path =  path + '/underexpose_test/local_2'\n",
    "\n",
    "\n",
    "def get_all_click(now_phase=6):\n",
    "    whole_all_click = pd.DataFrame()\n",
    "    for phase in range(now_phase + 1):\n",
    "#         print('phase: ', phase)\n",
    "\n",
    "        train_click = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(phase), names=['user_id', 'item_id', 'time'])\n",
    "        test_click = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(phase), names=['user_id', 'item_id', 'time'])\n",
    "\n",
    "        all_click = train_click.append(test_click)\n",
    "        whole_all_click = whole_all_click.append(all_click)\n",
    "        whole_all_click = whole_all_click.drop_duplicates(subset=['user_id', 'item_id', 'time'], keep='last')\n",
    "#         print(len(whole_all_click))\n",
    "\n",
    "    return whole_all_click\n",
    "\n",
    "\n",
    "\n",
    "def local_constr(phase, now_phase):\n",
    "    item_feat = pd.read_csv('/root/kdd_cup_2020/KDD_online_LB/data/item_feat_normalize.csv')\n",
    "    item_feat_dict = item_embedding_dict(item_feat)\n",
    "    all_click = get_all_click(now_phase = now_phase)\n",
    "    print(all_click.shape)\n",
    "    all_click = all_click.drop_duplicates(subset=['user_id', 'item_id', 'time'], keep='last')\n",
    "    all_click = all_click.sort_values(['user_id', 'time']).reset_index(drop=True)  \n",
    "    print(all_click.shape)\n",
    "    return item_feat, item_feat_dict, all_click\n",
    "\n",
    "def item_embedding_dict(df):\n",
    "    emb_feats = list(df.columns)[1:]\n",
    "    item_emb_dict = dict(zip(df['item_id'], df[emb_feats].values))\n",
    "    return item_emb_dict\n",
    "\n",
    "\n",
    "# fill user to 50 items\n",
    "def get_predict(df, pred_col, top_fill):\n",
    "    top_fill = [int(t) for t in top_fill.split(',')]\n",
    "    scores = [-1 * i for i in range(1, len(top_fill) + 1)]\n",
    "    ids = list(df['user_id'].unique())\n",
    "    fill_df = pd.DataFrame(ids * len(top_fill), columns=['user_id'])\n",
    "    fill_df.sort_values('user_id', inplace=True)\n",
    "    fill_df['item_id'] = top_fill * len(ids)\n",
    "    fill_df[pred_col] = scores * len(ids)\n",
    "    df = df.append(fill_df)\n",
    "    df.sort_values(pred_col, ascending=False, inplace=True)\n",
    "    df = df.drop_duplicates(subset=['user_id', 'item_id'], keep='first')\n",
    "    df['rank'] = df.groupby('user_id')[pred_col].rank(method='first', ascending=False)\n",
    "    df = df[df['rank'] <= 50]\n",
    "    df = df.groupby('user_id')['item_id'].apply(lambda x: ','.join([str(i) for i in x])).str.split(',',\n",
    "                                                                                                   expand=True).reset_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# the higher scores, the better performance\n",
    "def evaluate_each_phase(predictions, answers):\n",
    "    list_item_degress = []\n",
    "    for user_id in answers:\n",
    "        item_id, item_degree = answers[user_id]\n",
    "        list_item_degress.append(item_degree)\n",
    "    list_item_degress.sort()\n",
    "    median_item_degree = list_item_degress[len(list_item_degress) // 2]\n",
    "\n",
    "    num_cases_full = 0.0\n",
    "    ndcg_50_full = 0.0\n",
    "    ndcg_50_half = 0.0\n",
    "    num_cases_half = 0.0\n",
    "    hitrate_50_full = 0.0\n",
    "    hitrate_50_half = 0.0\n",
    "    for user_id in answers:\n",
    "        item_id, item_degree = answers[user_id]\n",
    "        rank = 0\n",
    "        while rank < 50 and predictions[user_id][rank] != item_id:\n",
    "            rank += 1\n",
    "        num_cases_full += 1.0\n",
    "        if rank < 50:\n",
    "            ndcg_50_full += 1.0 / np.log2(rank + 2.0)\n",
    "            hitrate_50_full += 1.0\n",
    "        if item_degree <= median_item_degree:\n",
    "            num_cases_half += 1.0\n",
    "            if rank < 50:\n",
    "                ndcg_50_half += 1.0 / np.log2(rank + 2.0)\n",
    "                hitrate_50_half += 1.0\n",
    "    ndcg_50_full /= num_cases_full\n",
    "    hitrate_50_full /= num_cases_full\n",
    "    ndcg_50_half /= num_cases_half\n",
    "    hitrate_50_half /= num_cases_half\n",
    "    return np.array([ndcg_50_full, ndcg_50_half,\n",
    "                     hitrate_50_full, hitrate_50_half], dtype=np.float32)\n",
    "\n",
    "def get_score_LB(test_label, test_pred, train_click, test_click):\n",
    "    assert len(test_label) == len(test_pred)\n",
    "    d = dict(pd.concat([train_click['item_id'], test_click['item_id']]).value_counts())\n",
    "    n = len(test_label)\n",
    "    \n",
    "    y_pred = {test_pred.iloc[i, 0]: list(test_pred.iloc[i, 1:]) for i in range(n)}\n",
    "    y_true = {test_label['user_id'][i]: \n",
    "              (test_label['item_id'][i], \n",
    "               d[test_label['item_id'][i]] if test_label['item_id'][i] in d else 0) \n",
    "              for i in range(n)}\n",
    "    \n",
    "    # ndcg_50_full, ndcg_50_half, hitrate_50_full, hitrate_50_half\n",
    "    LB = evaluate_each_phase(y_pred, y_true)\n",
    "    print('[+] NDCG@FULL = {:.6f}, NDCG@HALF = {:.6f}\\n[+]  HIT@FULL = {:.6f},  HIT@HALF = {:.6f}'.\\\n",
    "          format(LB[0], LB[1], LB[2], LB[3]))\n",
    "    return LB\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "def item_df_init(df, vals):\n",
    "    df_item_emb = pd.DataFrame([])\n",
    "    df_item_emb['item_id'] = df['item_id']\n",
    "    df_item_emb['item_emb'] = vals\n",
    "    df_item_emb['hash_item_id'] = df_item_emb.index\n",
    "    id2item_dic = dict(zip(df_item_emb['hash_item_id'], df_item_emb['item_id']))\n",
    "    item2id_vec = dict(zip(df_item_emb['item_id'], df_item_emb['item_emb']))\n",
    "\n",
    "    cur_dim = len(df_item_emb.iloc[0][1])\n",
    "    print('current dimension: {}'.format(cur_dim))\n",
    "\n",
    "\n",
    "    index = faiss.IndexFlatIP(cur_dim) # inner product\n",
    "    # index = faiss.IndexFlatL2(cur_dim)\n",
    "\n",
    "    item_embs = np.array(list(df_item_emb['item_emb'].values)).astype('float32')\n",
    "    faiss.normalize_L2(item_embs)\n",
    "    index.add(item_embs)\n",
    "    return df_item_emb, id2item_dic, item2id_vec, index\n",
    "\n",
    "\n",
    "df_item_emb_list = []\n",
    "id2item_dic_list = []\n",
    "item2id_vec_list = []\n",
    "index_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "def glove_init(all_click):\n",
    "    df = all_click.sort_values('time').reset_index(drop = True)\n",
    "    df['item_id'] = df['item_id'].astype(str)\n",
    "    user_item = df.groupby('user_id')['item_id'].agg(list).reset_index()\n",
    "    corpus = Corpus() \n",
    "    corpus.fit(user_item['item_id'].values, window=20)\n",
    "    glove_model = Glove(no_components = 300, learning_rate=0.1,random_state = 10)\n",
    " \n",
    "    glove_model.fit(corpus.matrix, epochs=100, no_threads=12, verbose=False)\n",
    "    glove_model.add_dictionary(corpus.dictionary)\n",
    "    tmp = pd.DataFrame(corpus.dictionary.keys(), columns = ['item_id']).astype(int)\n",
    "    vals = pd.DataFrame(glove_model.word_vectors).apply(lambda x: np.array(x).astype('float32'), axis = 1)\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = item_df_init(tmp, vals)\n",
    "    df_item_emb_list.append(df_item_emb)\n",
    "    id2item_dic_list.append(id2item_dic)\n",
    "    item2id_vec_list.append(item2id_vec)\n",
    "    index_list.append(index)\n",
    "    return df_item_emb, id2item_dic, item2id_vec, index\n",
    "\n",
    "def item_feat_full_init(item_feat):\n",
    "    tmp = pd.DataFrame(item_feat['item_id']).astype(int)\n",
    "    vals = item_feat.iloc[:, 1:].apply(lambda x: np.array(x).astype('float32'), axis = 1)\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = item_df_init(tmp, vals)\n",
    "    df_item_emb_list.append(df_item_emb)\n",
    "    id2item_dic_list.append(id2item_dic)\n",
    "    item2id_vec_list.append(item2id_vec)\n",
    "    index_list.append(index)\n",
    "    return df_item_emb, id2item_dic, item2id_vec, index\n",
    "\n",
    "def item_feat_text_init(item_feat):\n",
    "    tmp = pd.DataFrame(item_feat['item_id']).astype(int)\n",
    "    vals = item_feat.loc[:,[col for col in item_feat.columns if 'txt' in col]].apply(lambda x: np.array(x).astype('float32'), axis = 1)\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = item_df_init(tmp, vals)\n",
    "    df_item_emb_list.append(df_item_emb)\n",
    "    id2item_dic_list.append(id2item_dic)\n",
    "    item2id_vec_list.append(item2id_vec)\n",
    "    index_list.append(index)\n",
    "    return df_item_emb, id2item_dic, item2id_vec, index\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "\n",
    "def word2vec_init(all_click):\n",
    "    df = all_click.sort_values('time').reset_index(drop = True)\n",
    "    df['item_id'] = df['item_id'].astype(str)\n",
    "    user_item = df.groupby('user_id')['item_id'].agg(list).reset_index()\n",
    "    model = Word2Vec(user_item['item_id'].values, size=100, window=10, min_count=1, workers=12,\n",
    "                     seed=1997, iter=50, sg=1, hs=1, compute_loss=True,\n",
    "                     # min_alpha=0.005\n",
    "                     )\n",
    "    my_dict = dict({})\n",
    "    for idx, key in enumerate(model.wv.vocab):\n",
    "        my_dict[key] = model.wv[key]\n",
    "\n",
    "    tmp = pd.DataFrame(my_dict.keys(), columns = ['item_id']).astype(int)\n",
    "    vals = pd.DataFrame(my_dict.values()).apply(lambda x: np.array(x).astype('float32'), axis = 1)\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = item_df_init(tmp, vals)\n",
    "    df_item_emb_list.append(df_item_emb)\n",
    "    id2item_dic_list.append(id2item_dic)\n",
    "    item2id_vec_list.append(item2id_vec)\n",
    "    index_list.append(index)\n",
    "    return df_item_emb, id2item_dic, item2id_vec, index\n",
    "\n",
    "def word2vec_w10_init(all_click):\n",
    "    df = all_click.sort_values('time').reset_index(drop = True)\n",
    "    df['item_id'] = df['item_id'].astype(str)\n",
    "    user_item = df.groupby('user_id')['item_id'].agg(list).reset_index()\n",
    "    model = Word2Vec(user_item['item_id'].values, size=100, window=10, min_count=1, workers=12,\n",
    "                     seed=1997, iter=50, sg=1, hs=1, compute_loss=True,\n",
    "                     # min_alpha=0.005\n",
    "                     )\n",
    "    my_dict = dict({})\n",
    "    for idx, key in enumerate(model.wv.vocab):\n",
    "        my_dict[key] = model.wv[key]\n",
    "\n",
    "    tmp = pd.DataFrame(my_dict.keys(), columns = ['item_id']).astype(int)\n",
    "    vals = pd.DataFrame(my_dict.values()).apply(lambda x: np.array(x).astype('float32'), axis = 1)\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = item_df_init(tmp, vals)\n",
    "    df_item_emb_list.append(df_item_emb)\n",
    "    id2item_dic_list.append(id2item_dic)\n",
    "    item2id_vec_list.append(item2id_vec)\n",
    "    index_list.append(index)\n",
    "    return df_item_emb, id2item_dic, item2id_vec, index\n",
    "\n",
    "\n",
    "def word2vec_split_init(all_click):\n",
    "    df = all_click.sort_values('time').reset_index(drop = True)\n",
    "    df['item_id'] = df['item_id'].astype(str)\n",
    "    df['group'] = np.floor(df['time'] * 10000 - 9800)\n",
    "    user_item = df.groupby(['user_id','group'])['item_id'].agg(list).reset_index()\n",
    "    model = Word2Vec(user_item['item_id'].values, size=100, window=10, min_count=1, workers=12,\n",
    "                     seed=1997, iter=50, sg=1, hs=1, compute_loss=True,\n",
    "                     # min_alpha=0.005\n",
    "                     )\n",
    "    my_dict = dict({})\n",
    "    for idx, key in enumerate(model.wv.vocab):\n",
    "        my_dict[key] = model.wv[key]\n",
    "    tmp = pd.DataFrame(my_dict.keys(), columns = ['item_id']).astype(int)\n",
    "    vals = pd.DataFrame(my_dict.values()).apply(lambda x: np.array(x).astype('float32'), axis = 1)\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = item_df_init(tmp, vals)\n",
    "    df_item_emb_list.append(df_item_emb)\n",
    "    id2item_dic_list.append(id2item_dic)\n",
    "    item2id_vec_list.append(item2id_vec)\n",
    "    index_list.append(index)\n",
    "    return df_item_emb, id2item_dic, item2id_vec, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_res(history_click, item2id_vec, id2item_dic, index, query_user, item_set):\n",
    "    ratio_up, ratio_base = 1, 1\n",
    "    pos_up = -0.5\n",
    "    topk = 100\n",
    "    recall_num = 500\n",
    "#     item_set = set(history_click['item_id'].unique())\n",
    "    item_time_list = history_click[['item_id', 'time']].values.tolist()\n",
    "    item_time_list = [[item, time] for item, time in item_time_list if item in item2id_vec.keys()]\n",
    "    vec_list = [item2id_vec[item] for item, _ in item_time_list if item in item2id_vec.keys()]\n",
    "    if len(vec_list) == 0:\n",
    "        return\n",
    "    vecs = np.stack(vec_list, axis=0).astype('float32')\n",
    "    faiss.normalize_L2(vecs)\n",
    "    D, I = index.search(vecs, topk)\n",
    "    cur_dic = {}\n",
    "    for i, (item, time) in enumerate(item_time_list):\n",
    "        ratio = ratio_up * 1.0 / (i + ratio_base) # (i + 1.) ** (-0.1)\n",
    "        for k, idx in enumerate(I[i, :]):\n",
    "            cur_item = id2item_dic[idx]\n",
    "            pos_score = ratio * ((k + 1) ** pos_up)\n",
    "                # pos_score = ratio * (1. / (k + 1.))\n",
    "            if cur_item not in item_set:\n",
    "                try:\n",
    "                    cur_dic[cur_item] += D[i, k] * pos_score\n",
    "                except KeyError as _:\n",
    "                    cur_dic[cur_item] = D[i, k] * pos_score\n",
    "    cur_sorted = sorted(cur_dic.items(), key = lambda x: x[1], reverse = True)\n",
    "    rec_df = pd.DataFrame(cur_sorted[:recall_num],columns = ['item_id', 'sim'])\n",
    "    rec_df['user_id'] = query_user\n",
    "    return rec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase:7\n",
      "(1243109, 3)\n",
      "(1243109, 3)\n",
      "1. glove.\n",
      "current dimension: 300\n",
      "2. item all.\n",
      "current dimension: 256\n",
      "3. text all.\n",
      "current dimension: 128\n",
      "4. word2vec.\n",
      "current dimension: 300\n",
      "5. word2vec 10w.\n",
      "current dimension: 300\n",
      "6. word2vec split.\n",
      "current dimension: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1701/1701 [08:15<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save result:\n",
      "phase:8\n",
      "(1243109, 3)\n",
      "(1243109, 3)\n",
      "1. glove.\n",
      "current dimension: 300\n",
      "2. item all.\n",
      "current dimension: 256\n",
      "3. text all.\n",
      "current dimension: 128\n",
      "4. word2vec.\n",
      "current dimension: 300\n",
      "5. word2vec 10w.\n",
      "current dimension: 300\n",
      "6. word2vec split.\n",
      "current dimension: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1716/1716 [08:39<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save result:\n",
      "phase:9\n",
      "(1243109, 3)\n",
      "(1243109, 3)\n",
      "1. glove.\n",
      "current dimension: 300\n",
      "2. item all.\n",
      "current dimension: 256\n",
      "3. text all.\n",
      "current dimension: 128\n",
      "4. word2vec.\n",
      "current dimension: 300\n",
      "5. word2vec 10w.\n",
      "current dimension: 300\n",
      "6. word2vec split.\n",
      "current dimension: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1651/1651 [08:28<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save result:\n"
     ]
    }
   ],
   "source": [
    "now_phase = 9\n",
    "file_save_path = path + '/local_result/'\n",
    "\n",
    "for phase in range(7, now_phase + 1):\n",
    "    df_item_emb_list = []\n",
    "    id2item_dic_list = []\n",
    "    item2id_vec_list = []\n",
    "    index_list = []\n",
    "    print(f\"phase:{phase}\")\n",
    "    item_feat, item_feat_dict, all_click = local_constr(phase, now_phase)\n",
    "    query = pd.read_csv(test_path + '/underexpose_test_qtime-{}.csv'.format(phase), names=['user_id', 'time'])\n",
    "    train_click = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(phase), names=['user_id', 'item_id', 'time'])\n",
    "    test_click = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(phase), names=['user_id', 'item_id', 'time'])\n",
    "    phase_click = pd.concat([train_click, test_click], sort = False, axis = 0).reset_index(drop = True)\n",
    "    \n",
    "    print(\"1. glove.\")\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = glove_init(all_click)\n",
    "    print(\"2. item all.\")\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = item_feat_full_init(item_feat)\n",
    "    print(\"3. text all.\")\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = item_feat_text_init(item_feat)  \n",
    "    print('4. word2vec.')\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = word2vec_init(phase_click)\n",
    "    print('5. word2vec 10w.')\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = word2vec_w10_init(all_click)\n",
    "    print('6. word2vec split.')\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = word2vec_split_init(all_click)\n",
    "    \n",
    "    df_list = [pd.DataFrame([])] * len(index_list)\n",
    "    for query_user, query_time in tqdm(query.values.tolist()):\n",
    "        history_click = all_click.loc[(all_click['user_id'] == query_user) & (all_click['time'] <= query_time)]\n",
    "        history_click = history_click.sort_values('time', ascending=False)\n",
    "        item_set = set(all_click.loc[(all_click['user_id'] == query_user), 'item_id'])\n",
    "        for i in range(len(df_list)):\n",
    "            tmp = partial_res(history_click, item2id_vec_list[i], id2item_dic_list[i], \n",
    "                              index_list[i], query_user, item_set)\n",
    "            df_list[i] = df_list[i].append(tmp)\n",
    "            \n",
    "    res_glove = df_list[0]\n",
    "    res_item_all = df_list[1]\n",
    "    res_item_txt = df_list[2]\n",
    "    res_w2v = df_list[3]\n",
    "    res_w2v_10w = df_list[4]\n",
    "    res_w2v_gr = df_list[5]\n",
    "\n",
    "    res1 = pd.merge(res_glove.rename(columns={\"sim\":\"sim_glove\"}), \n",
    "                res_item_all.rename(columns={\"sim\":\"sim_all\"}), on = ['user_id', 'item_id'], how = 'outer')\n",
    "    res1 = pd.merge(res1, res_item_txt.rename(columns={\"sim\":\"sim_txt\"}), on = ['user_id', 'item_id'], how = 'outer')\n",
    "    res1 = pd.merge(res1, res_w2v.rename(columns={\"sim\":\"sim_w2v\"}), on = ['user_id', 'item_id'], how = 'outer')\n",
    "    res1 = pd.merge(res1, res_w2v_10w.rename(columns={\"sim\":\"sim_w2v_10w\"}), on = ['user_id', 'item_id'], how = 'outer')\n",
    "    res1 = pd.merge(res1, res_w2v_gr.rename(columns={\"sim\":\"sim_w2v_gr\"}), on = ['user_id', 'item_id'], how = 'outer')\n",
    "\n",
    "\n",
    "    for col in ['sim_glove', 'sim_all', 'sim_txt', 'sim_w2v', 'sim_w2v_10w', 'sim_w2v_gr']:\n",
    "        min_score = res1.groupby(['user_id'])[col].transform('min')\n",
    "        max_score = res1.groupby(['user_id'])[col].transform('max') \n",
    "        res1[col] = \\\n",
    "        (res1[col] - min_score)/(max_score - min_score)\n",
    "    res1.fillna(0, inplace = True)\n",
    "    res1['score'] = \\\n",
    "            (res1['sim_glove'] + res1['sim_all'] + res1['sim_txt']) * 0.3 + \\\n",
    "            (res1['sim_w2v'] + res1['sim_w2v_10w']) * 0.7 + res1['sim_w2v_gr'] * 0.2\n",
    "    print(\"save result:\")\n",
    "    if not os.path.exists(file_save_path):\n",
    "        os.mkdir(file_save_path)\n",
    "    res1.to_csv(file_save_path + f'/hybrid_emb_update_v3_test_phase{phase}.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def local_train_constr(phase, now_phase):\n",
    "    whole_click = pd.DataFrame()\n",
    "    for c in range(0, now_phase + 1):\n",
    "        recom_item = []\n",
    "        click_train = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "        click_test = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])\n",
    "        all_click = click_train.append(click_test)\n",
    "        all_click = all_click.sort_values(['user_id', 'time']).reset_index(drop = True)\n",
    "        whole_click = whole_click.append(all_click)\n",
    "    \n",
    "    click_train = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(phase), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "    click_test = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(phase), header=None,  names=['user_id', 'item_id', 'time'])\n",
    "    all_click = click_train.append(click_test).reset_index(drop = True)\n",
    "    print(\"phase series:\", all_click.shape)\n",
    "    all_click = all_click.sort_values(['user_id', 'time']).reset_index(drop = True)\n",
    "    train_answer = all_click.groupby(['user_id']).last().reset_index()\n",
    "    all_click.set_index(['user_id', 'item_id'], inplace = True)\n",
    "    all_click = all_click[~all_click.index.isin(train_answer.set_index(['user_id', 'item_id']).index)].reset_index()\n",
    "    print(\"phase series:\", all_click.shape)\n",
    "    \n",
    "    whole_click = whole_click.drop_duplicates(['user_id', 'item_id', 'time']).reset_index(drop = True)\n",
    "    print(\"all click:\",whole_click.shape)\n",
    "    whole_click.set_index(['user_id', 'item_id'], inplace = True)\n",
    "    whole_click = whole_click[~whole_click.index.isin(train_answer.set_index(['user_id', 'item_id']).index)].reset_index()\n",
    "    whole_click = whole_click.sort_values(['user_id', 'time']).reset_index(drop = True)\n",
    "    print(\"all click:\",whole_click.shape)\n",
    "    \n",
    "    item_feat = pd.read_csv('/root/kdd_cup_2020/KDD_online_LB/data/item_feat_normalize.csv')\n",
    "    item_feat_dict = item_embedding_dict(item_feat)  \n",
    "    return item_feat, item_feat_dict, whole_click, all_click, train_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train phase:7\n",
      "phase series: (294763, 3)\n",
      "phase series: (275093, 3)\n",
      "all click: (1243109, 3)\n",
      "all click: (1218677, 3)\n",
      "1. glove.\n",
      "current dimension: 300\n",
      "2. item all.\n",
      "current dimension: 256\n",
      "3. text all.\n",
      "current dimension: 128\n",
      "4. word2vec.\n",
      "current dimension: 300\n",
      "5. word2vec 10w.\n",
      "current dimension: 300\n",
      "6. word2vec split.\n",
      "current dimension: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19670/19670 [2:07:07<00:00,  2.21it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save result:\n"
     ]
    }
   ],
   "source": [
    "now_phase = 9\n",
    "for phase in range(7, now_phase + 1):\n",
    "    df_item_emb_list = []\n",
    "    id2item_dic_list = []\n",
    "    item2id_vec_list = []\n",
    "    index_list = []\n",
    "    print(f\"train phase:{phase}\")\n",
    "    item_feat, item_feat_dict, all_click, phase_click, train_answer = local_train_constr(phase, now_phase)\n",
    "    query = train_answer[['user_id', 'time']]    \n",
    "    \n",
    "    print(\"1. glove.\")\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = glove_init(all_click)\n",
    "    print(\"2. item all.\")\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = item_feat_full_init(item_feat)\n",
    "    print(\"3. text all.\")\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = item_feat_text_init(item_feat)  \n",
    "    print('4. word2vec.')\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = word2vec_init(phase_click)\n",
    "    print('5. word2vec 10w.')\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = word2vec_w10_init(all_click)\n",
    "    print('6. word2vec split.')\n",
    "    df_item_emb, id2item_dic, item2id_vec, index = word2vec_split_init(all_click)\n",
    "        \n",
    "    df_list = [pd.DataFrame([])] * len(index_list)\n",
    "    for query_user, query_time in tqdm(query.values.tolist()):\n",
    "        history_click = all_click.loc[(all_click['user_id'] == query_user) & (all_click['time'] <= query_time)]\n",
    "        history_click = history_click.sort_values('time', ascending=False)\n",
    "        item_set = set(all_click.loc[(all_click['user_id'] == query_user), 'item_id'])\n",
    "        for i in range(len(df_list)):\n",
    "            tmp = partial_res(history_click, item2id_vec_list[i], id2item_dic_list[i], \n",
    "                              index_list[i], query_user, item_set)\n",
    "            df_list[i] = df_list[i].append(tmp)\n",
    "            \n",
    "    res_glove = df_list[0]\n",
    "    res_item_all = df_list[1]\n",
    "    res_item_txt = df_list[2]\n",
    "    res_w2v = df_list[3]\n",
    "    res_w2v_10w = df_list[4]\n",
    "    res_w2v_gr = df_list[5]\n",
    "\n",
    "    res1 = pd.merge(res_glove.rename(columns={\"sim\":\"sim_glove\"}), \n",
    "                res_item_all.rename(columns={\"sim\":\"sim_all\"}), on = ['user_id', 'item_id'], how = 'outer')\n",
    "    res1 = pd.merge(res1, res_item_txt.rename(columns={\"sim\":\"sim_txt\"}), on = ['user_id', 'item_id'], how = 'outer')\n",
    "    res1 = pd.merge(res1, res_w2v.rename(columns={\"sim\":\"sim_w2v\"}), on = ['user_id', 'item_id'], how = 'outer')\n",
    "    res1 = pd.merge(res1, res_w2v_10w.rename(columns={\"sim\":\"sim_w2v_10w\"}), on = ['user_id', 'item_id'], how = 'outer')\n",
    "    res1 = pd.merge(res1, res_w2v_gr.rename(columns={\"sim\":\"sim_w2v_gr\"}), on = ['user_id', 'item_id'], how = 'outer')\n",
    "\n",
    "\n",
    "    for col in ['sim_glove', 'sim_all', 'sim_txt', 'sim_w2v', 'sim_w2v_10w', 'sim_w2v_gr']:\n",
    "        min_score = res1.groupby(['user_id'])[col].transform('min')\n",
    "        max_score = res1.groupby(['user_id'])[col].transform('max') \n",
    "        res1[col] = \\\n",
    "        (res1[col] - min_score)/(max_score - min_score)\n",
    "    res1.fillna(0, inplace = True)\n",
    "    res1['score'] = \\\n",
    "            (res1['sim_glove'] + res1['sim_all'] + res1['sim_txt']) * 0.3 + \\\n",
    "            (res1['sim_w2v'] + res1['sim_w2v_10w']) * 0.7 + res1['sim_w2v_gr'] * 0.2\n",
    "    print(\"save result:\")\n",
    "    if not os.path.exists(file_save_path):\n",
    "        os.mkdir(file_save_path)\n",
    "    res1.to_csv(file_save_path + f'/hybrid_emb_update_v3_train_phase{phase}.csv', index = None)\n",
    "\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
