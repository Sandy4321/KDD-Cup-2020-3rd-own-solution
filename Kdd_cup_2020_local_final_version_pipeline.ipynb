{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "from tqdm import tqdm  \n",
    "from collections import defaultdict  \n",
    "import math  \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "path = '/root/kdd_cup_2020'\n",
    "train_path = path + '/underexpose_train/local_2'  \n",
    "test_path =  path + '/underexpose_test/local_2'  \n",
    "now_phase = 9\n",
    "recall_num = 50\n",
    "\n",
    "\n",
    "\n",
    "def rec_wrap(idx, recall_num):\n",
    "    recom_item = []\n",
    "    rank_item = recommend(cut_dict, item_sim_list, user_item_list, idx, 500, recall_num)\n",
    "    for j in rank_item:\n",
    "        recom_item.append([idx, j[0], j[1]])\n",
    "    return recom_item\n",
    "\n",
    "\n",
    "def get_predict(df, pred_col, top_fill, rank_num):  \n",
    "    top_fill = [int(t) for t in top_fill.split(',')]  \n",
    "    scores = [-1 * i for i in range(1, len(top_fill) + 1)]  \n",
    "    ids = list(df['user_id'].unique())  \n",
    "    fill_df = pd.DataFrame(ids * len(top_fill), columns=['user_id'])  \n",
    "    fill_df.sort_values('user_id', inplace=True)  \n",
    "    fill_df['item_id'] = top_fill * len(ids)  \n",
    "    fill_df[pred_col] = scores * len(ids)  \n",
    "    df = df.append(fill_df)  \n",
    "    df.sort_values(pred_col, ascending=False, inplace=True)  \n",
    "    df = df.drop_duplicates(subset=['user_id', 'item_id'], keep='first')  \n",
    "    df['rank'] = df.groupby('user_id')[pred_col].rank(method='first', ascending=False)  \n",
    "    df = df[df['rank'] <= rank_num]  \n",
    "    df = df.groupby('user_id')['item_id'].apply(lambda x: ','.join([str(i) for i in x])).str.split(',', expand=True).reset_index()  \n",
    "    return df\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def ndcg_metric(tmp):\n",
    "    ndcg = 0\n",
    "    for rank in range(50):\n",
    "        ndcg += np.sum((tmp[rank].astype(int) == tmp['item_id'])/np.log2(rank + 2))\n",
    "    ndcg /= tmp.shape[0]\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def get_sim_item(df_, user_col, item_col, use_iif=False): \n",
    "    df = df_.copy()\n",
    "    user_item_ = df.groupby(user_col)[item_col].agg(list).reset_index()\n",
    "    user_item_dict = dict(zip(user_item_[user_col], user_item_[item_col]))\n",
    "\n",
    "    user_time_ = df.groupby(user_col)['time'].agg(list).reset_index() # 引入时间因素\n",
    "    user_time_dict = dict(zip(user_time_[user_col], user_time_['time']))\n",
    "\n",
    "    sim_item = {}  \n",
    "    item_cnt = defaultdict(int)  # 商品被点击次数\n",
    "    for user, items in tqdm(user_item_dict.items()):  \n",
    "        for loc1, item in enumerate(items):  \n",
    "            item_cnt[item] += 1  \n",
    "            sim_item.setdefault(item, {})  \n",
    "            for loc2, relate_item in enumerate(items):  \n",
    "                if item == relate_item:  \n",
    "                    continue  \n",
    "                t1 = user_time_dict[user][loc1] # 点击时间提取\n",
    "                t2 = user_time_dict[user][loc2]\n",
    "                sim_item[item].setdefault(relate_item, 0)  \n",
    "                if not use_iif:\n",
    "                    corr = 1\n",
    "                        \n",
    "                    constant = math.log(1 + len(items))\n",
    "                    if loc1-loc2>0:\n",
    "                        sim_item[item][relate_item] += \\\n",
    "                        1 * 0.7 * (0.8**(loc1-loc2-1)) * (1 - (t1 - t2) * 5000) / constant\n",
    "                    else:\n",
    "                        sim_item[item][relate_item] += \\\n",
    "                        1 * 0.7 * (0.8**(loc2-loc1-1)) * (1 - (t2 - t1) * 5000) / constant\n",
    "                else:\n",
    "                    sim_item[item][relate_item] += 1 / math.log(1 + len(items))\n",
    "\n",
    "    sim_item_corr = sim_item.copy() # 引入AB的各种被点击次数  \n",
    "    for i, related_items in tqdm(sim_item.items()):  \n",
    "        for j, cij in related_items.items():  \n",
    "            sim_item_corr[i][j] = cij / ((item_cnt[i] * item_cnt[j]) ** 0.2)  \n",
    "    return sim_item_corr, user_item_dict\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "def recommend(cut_dict, sim_item_corr, user_item_dict, user_id, top_k, item_num):  \n",
    "    '''\n",
    "    input:item_sim_list, user_item, uid, 500, 50\n",
    "    # 用户历史序列中的所有商品均有关联商品,整合这些关联商品,进行相似性排序\n",
    "    '''\n",
    "    rank = {}\n",
    "    item_all = cut_dict[user_id]\n",
    "#     item_num = min(item_num, 100)\n",
    "    interacted_items = user_item_dict[user_id]\n",
    "    item_drop = deepcopy(interacted_items)\n",
    "    interacted_items = interacted_items[:item_all][::-1]\n",
    "    \n",
    "    for loc, i in enumerate(interacted_items):  \n",
    "#         \n",
    "        for j, wij in sorted(sim_item_corr[i].items(),key = lambda d: d[1], reverse=True)[0:top_k]:  \n",
    "            if j not in item_drop:  \n",
    "                rank.setdefault(j, 0)  \n",
    "                rank[j] += wij * (0.5**loc)\n",
    "    return sorted(rank.items(), key=lambda d: d[1], reverse=True)[:item_num]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def df_constr(whole_click, qtime):\n",
    "    top50_click = whole_click['item_id'].value_counts().index[:recall_num].values  \n",
    "    top50_click = ','.join([str(i) for i in top50_click])\n",
    "\n",
    "    click_cut = whole_click[['user_id', 'time']].copy()\n",
    "    click_cut = click_cut.loc[click_cut['user_id'].isin(qtime['user_id'])].reset_index(drop = True)\n",
    "    qtime_dic = dict(qtime.values)\n",
    "    click_cut['qtime'] = click_cut['user_id'].apply(lambda x: qtime_dic[x] )\n",
    "    click_cut = click_cut.loc[(click_cut['time'] <= click_cut['qtime'])].reset_index(drop = True)\n",
    "\n",
    "    cut_item = click_cut.groupby(['user_id'])[['time']].count().reset_index()\n",
    "    cut_dict = dict(cut_item.values)\n",
    "    return top50_click, cut_dict\n",
    "\n",
    "def recommend_pars(top50_click, qtime, recall_num):\n",
    "    res = Parallel(n_jobs = 12, backend = 'multiprocessing')\\\n",
    "              (delayed(rec_wrap)(idx, recall_num)\n",
    "              for idx in tqdm(qtime['user_id'].unique())\n",
    "              )\n",
    "    recom_item = []\n",
    "    for i in range(len(res)):\n",
    "        recom_item += res[i]\n",
    "    recom_df = pd.DataFrame(recom_item, columns=['user_id', 'item_id', 'sim'])\n",
    "    result = get_predict(recom_df, 'sim', top50_click, recall_num)\n",
    "    return result, recom_df\n",
    "\n",
    "def local_test_constr(phase, now_phase, recall_num, path = path):\n",
    "    whole_click = pd.DataFrame()\n",
    "    test_answer = pd.DataFrame([])\n",
    "    for c in range(0, now_phase + 1): \n",
    "        recom_item = []\n",
    "        click_train = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "        click_test = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])\n",
    "        all_click = click_train.append(click_test)\n",
    "        whole_click = whole_click.append(all_click)\n",
    "    \n",
    "    whole_click = whole_click.drop_duplicates(['user_id', 'item_id', 'time']).reset_index(drop = True)\n",
    "    whole_click = whole_click.sort_values(['user_id', 'time']).reset_index(drop = True)\n",
    "    \n",
    "    click_train = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(phase), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "    click_test = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(phase), header=None,  names=['user_id', 'item_id', 'time'])\n",
    "    qtime = pd.read_csv(test_path + '/underexpose_test_qtime-{}.csv'.format(phase), header=None,  names=['user_id', 'time'])\n",
    "    all_click = click_train.append(click_test).reset_index(drop = True)\n",
    "    return whole_click, all_click, qtime\n",
    "\n",
    "def local_train_constr(phase, now_phase, recall_num, path = path):\n",
    "    whole_click = pd.DataFrame()\n",
    "    for c in range(0, now_phase + 1):\n",
    "        recom_item = []\n",
    "        click_train = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "        click_test = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])\n",
    "        all_click = click_train.append(click_test)\n",
    "        all_click = all_click.sort_values(['user_id', 'time']).reset_index(drop = True)\n",
    "        whole_click = whole_click.append(all_click)\n",
    "    \n",
    "    click_train = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(phase), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "    click_test = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(phase), header=None,  names=['user_id', 'item_id', 'time'])\n",
    "    all_click = click_train.append(click_test).reset_index(drop = True)\n",
    "    print(\"phase series:\", all_click.shape)\n",
    "    all_click = all_click.sort_values(['user_id', 'time']).reset_index(drop = True)\n",
    "    train_answer = all_click.groupby(['user_id']).last().reset_index()\n",
    "    all_click.set_index(['user_id', 'item_id'], inplace = True)\n",
    "    all_click = all_click[~all_click.index.isin(train_answer.set_index(['user_id', 'item_id']).index)].reset_index()\n",
    "    print(\"phase series:\", all_click.shape)\n",
    "    \n",
    "    whole_click = whole_click.drop_duplicates(['user_id', 'item_id', 'time']).reset_index(drop = True)\n",
    "    print(\"all click:\",whole_click.shape)\n",
    "    whole_click.set_index(['user_id', 'item_id'], inplace = True)\n",
    "    whole_click = whole_click[~whole_click.index.isin(train_answer.set_index(['user_id', 'item_id']).index)].reset_index()\n",
    "    whole_click = whole_click.sort_values(['user_id', 'time']).reset_index(drop = True)\n",
    "    print(\"all click:\",whole_click.shape)\n",
    "    return whole_click, all_click, train_answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "params = {\n",
    "            'num_leaves':  63,\n",
    "#           'min_child_weight': 0.034,\n",
    "          'feature_fraction': 0.5,\n",
    "          'bagging_fraction':  0.5,\n",
    "#           'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'random_state': 47\n",
    "         } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_input_root = '/root/kdd_cup_2020'\n",
    "path_input_train = path_input_root + '/underexpose_train/local_2'  \n",
    "path_input_test = path_input_root + '/underexpose_test/local_2'  \n",
    "path_input_train_feat = '../input/underexpose_train/' # 这个LB和LOCAL用一样的就行\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import pandas as pd  \n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict  \n",
    "import math  \n",
    "\n",
    "# ================================================================\n",
    "W1 = -0.3\n",
    "W2 = 30\n",
    "W3 = 2.7182818 \n",
    "W1_list = [-0.15, -0.2, -0.25, -0.3, -0.35, -0.4]\n",
    "W2_list = [1, 2, 5, 10, 20, 25, 30]\n",
    "W3_list = [2,  2.7182818, 5]\n",
    "\n",
    "W4 = 10\n",
    "W5 = 14\n",
    "W4_list = [1, 2, 5, 10, 20]\n",
    "W5_list = [2, 5, 8, 10, 14, 18, 22]\n",
    "\n",
    "W4 = 25\n",
    "W5 = 10\n",
    "W6 = 0.4\n",
    "\n",
    "def get_sim_item_runxing(df_, user_col, item_col, use_iif = False):\n",
    "    df = df_.copy()\n",
    "    user_item_ = df.groupby(user_col)[item_col].agg(list).reset_index()\n",
    "    user_item_dict = dict(zip(user_item_[user_col], user_item_[item_col]))\n",
    "\n",
    "    sim_item = {}\n",
    "    item_cnt = defaultdict(int)\n",
    "    for user, items in tqdm(user_item_dict.items()):\n",
    "    # for user, items in user_item_dict.items():\n",
    "        for idx_cur, i in enumerate(items):\n",
    "            item_cnt[i] += 1\n",
    "            sim_item.setdefault(i, {})\n",
    "            for idx_relate, relate_item in enumerate(items):\n",
    "                if i == relate_item:\n",
    "                    continue\n",
    "                sim_item[i].setdefault(relate_item, 0)\n",
    "                w_time_inner = abs(idx_cur - idx_relate) ** (W1)\n",
    "                if not use_iif:\n",
    "                    sim_item[i][relate_item] += 1\n",
    "                else:\n",
    "                    sim_item[i][relate_item] += 1 / math.log(W2 + len(items), W3) * w_time_inner\n",
    "\n",
    "    sim_item_corr = sim_item.copy()\n",
    "    for i, related_items in tqdm(sim_item.items()):\n",
    "    # for i, related_items in (sim_item.items()):\n",
    "        for j, cij in related_items.items():\n",
    "            D = (math.sqrt(item_cnt[i] * item_cnt[j]) - (np.log(W4 + W6 * abs(item_cnt[i] - item_cnt[j])) - W5))\n",
    "            sim_item_corr[i][j] = cij / D\n",
    "\n",
    "    return sim_item_corr, user_item_dict\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "R1 = -0.5\n",
    "R2 = -0.1\n",
    "R1_list = [-0.1, -0.2, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8, -0.9]\n",
    "R2_list = [-0.005, -0.01, -0.03, -0.05, -0.08, -0.1]\n",
    "\n",
    "R3 = 30\n",
    "R4 = 5\n",
    "R3_list = [1, 5,10, 20, 30, 40, 50, 100, 200]\n",
    "R4_list = [0.1, 0.5, 1, 2, 5, 10]\n",
    "\n",
    "R5 = 0.01\n",
    "R6 = 0\n",
    "R7 = 1\n",
    "R5_list = [1, 5, 10, 20, 30, 50]\n",
    "R6_list = [1, 2, 5, 10, 15, 20]\n",
    "R7_list = [0.1, 0.5, 1, 2, 5, 10]\n",
    "\n",
    "def recommend_runxing(T, NOT_item_dict, sim_item_corr, user_item_dict, user_id, top_k, item_num):\n",
    "    rank = {}\n",
    "    interacted_items = user_item_dict[user_id]\n",
    "    # interacted_items  = [item for item in interacted_items if item not in NOT_item_dict[user_id]]\n",
    "    \n",
    "    interacted_items_set = set(interacted_items) # @zhongrunxing\n",
    "    length = len(interacted_items)\n",
    "    for idx, i in enumerate(interacted_items):\n",
    "        w_time = ((length - idx) ** (R1))\n",
    "        ni = 0\n",
    "        for j, wij in sim_item_corr[i][0:top_k]:\n",
    "            if j in NOT_item_dict[user_id]:\n",
    "                continue;\n",
    "                \n",
    "            if j not in interacted_items_set:\n",
    "                ni += 1\n",
    "                w_item_rank = ni ** (R2)\n",
    "                rank.setdefault(j, 0)\n",
    "                \n",
    "                # 需要另一个逻辑, 表麻麻烦暂时, @zhongrunxing\n",
    "#                 try:\n",
    "#                     sim_feat = sim_dict_feat[i][j]\n",
    "#                 except:\n",
    "#                     sim_feat = 0\n",
    "#                 wij = wij * (math.log(R3 * (sim_feat - sim_min_list[T]) + 1, 2)) ** R4\n",
    "                \n",
    "                # 针对half, @zhongrunxing\n",
    "                # wij = wij * (1 / np.log1p(R5 * item_vc_list[T][j] + R6)) ** R7\n",
    "                \n",
    "                rank[j] += wij * w_time * w_item_rank # * (item_vc_list[T][j] ** 0.05)\n",
    "                \n",
    "    rank_item = sorted(rank.items(), key=lambda d: d[1], reverse=True)[:item_num]        \n",
    "    return rank_item\n",
    "\n",
    "def get_predict_runxing(df, pred_col, item_top_list, N_recall = 50):  \n",
    "    # item_top\n",
    "    scores = [-1 * i for i in range(1, len(item_top_list) + 1)]  \n",
    "    ids = list(df['user_id'].unique())  \n",
    "    fill_df = pd.DataFrame(ids * len(item_top_list), columns=['user_id'])  \n",
    "    fill_df.sort_values('user_id', inplace=True)  \n",
    "    fill_df['item_id'] = item_top_list * len(ids)  \n",
    "    fill_df[pred_col] = scores * len(ids)  \n",
    "    df = df.append(fill_df)  \n",
    "    \n",
    "    df.sort_values(pred_col, ascending=False, inplace=True)  \n",
    "    df = df.drop_duplicates(subset=['user_id', 'item_id'], keep='first')  \n",
    "    df['rank'] = df.groupby('user_id')[pred_col].rank(method='first', ascending=False).astype(int)\n",
    "    \n",
    "    # df_score = df.copy()\n",
    "    df = df[df['rank'] <= N_recall]  \n",
    "    df_score = df.copy()\n",
    "    \n",
    "    df_score = df_score.sort_values(by = ['user_id', 'rank']).reset_index(drop = True)\n",
    "    df = df.groupby('user_id')['item_id'].apply(lambda x: ','.join([str(i) for i in x])).\\\n",
    "        str.split(',', expand=True).reset_index()  \n",
    "    \n",
    "    for col in df:\n",
    "        df[col] = df[col].astype(float).astype(int)\n",
    "    return df, df_score\n",
    "\n",
    "def CF_train(T, train_test_click):\n",
    "    item_sim_list, user_item = get_sim_item_runxing(train_test_click, 'user_id', 'item_id', use_iif = True) \n",
    "    sim_item_corr = {}\n",
    "    for key in list(item_sim_list.keys()):    \n",
    "        item_neighbour_list = sorted(item_sim_list[key].items(), key = lambda x: x[1], reverse = True)\n",
    "        sim_item_corr[key] = item_neighbour_list\n",
    "    return sim_item_corr, user_item\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def rec_wrap_runxing(user_id, T, N_recall):\n",
    "    user_item_sim = []\n",
    "    rank_item = recommend_runxing(T, NOT_item_dict, sim_item_corr, user_item, user_id, 500, N_recall)  \n",
    "    for j in rank_item:\n",
    "        user_item_sim.append([user_id, j[0], j[1]])\n",
    "    return user_item_sim\n",
    "\n",
    "def CF_predict(train_click_i, test_click_i, T, NOT_item_dict, test_qtime_i, sim_item_corr, user_item, N_recall = 50):\n",
    "    res = Parallel(n_jobs = 12, backend = 'multiprocessing')\\\n",
    "              (delayed(rec_wrap_runxing)(test_qtime_i.iloc[i]['user_id'], T, 50)\n",
    "              for i in tqdm(range(test_qtime_i.shape[0]))\n",
    "              )\n",
    "    recom_item_test = []\n",
    "    for i in range(len(res)):\n",
    "        recom_item_test += res[i]\n",
    "        \n",
    "    recom_df_test = pd.DataFrame(recom_item_test, columns=['user_id', 'item_id', 'sim'])  \n",
    "    item_all = pd.concat([train_click_i['item_id'], test_click_i['item_id']])\n",
    "    item_top_list = list(item_all.value_counts().index[:N_recall].values)\n",
    "    df_pred_test, df_score_test = get_predict_runxing(recom_df_test, 'sim', item_top_list, N_recall)  \n",
    "    return df_pred_test, df_score_test\n",
    "\n",
    "\n",
    "def local_test_constr_RUNXING(Ti, now_phase, N_recall = 50):\n",
    "    train_click_list = []\n",
    "    test_click_list = []\n",
    "    test_qtime_list = []\n",
    "\n",
    "    for i in range(now_phase + 1):\n",
    "        train_click_i = pd.read_csv(path_input_train + '/' + 'underexpose_train_click-{}.csv'.format(i), \n",
    "                                   header = None, names=['user_id', 'item_id', 'time'])\n",
    "        test_click_i = pd.read_csv(path_input_test + '/' + 'underexpose_test_click-{}.csv'.format(i), \n",
    "                                   header = None, names=['user_id', 'item_id', 'time'])\n",
    "        test_qtime_i = pd.read_csv(path_input_test + '/' + 'underexpose_test_qtime-{}.csv'.format(i), \n",
    "                                   header = None, names=['user_id', 'time'])\n",
    "    \n",
    "        train_click_i.sort_values(['user_id', 'time'], inplace = True)\n",
    "        test_click_i.sort_values(['user_id', 'time'], inplace = True)\n",
    "        test_qtime_i.sort_values(['user_id', 'time'], inplace = True)\n",
    "    \n",
    "        train_click_list.append(train_click_i)\n",
    "        test_click_list.append(test_click_i)\n",
    "        test_qtime_list.append(test_qtime_i)\n",
    "        \n",
    "    train_click_ALL = pd.concat(train_click_list, axis = 0)\n",
    "    test_click_ALL = pd.concat(test_click_list, axis = 0)\n",
    "    train_test_click_ALL = train_click_ALL.append(test_click_ALL)\n",
    "    train_test_click_ALL.sort_values(['user_id', 'time'], inplace = True)\n",
    "    train_test_click_ALL.drop_duplicates(['user_id', 'item_id'], keep = 'last', inplace = True)\n",
    "    train_test_click_ALL.reset_index(drop = True,  inplace = True)\n",
    "    \n",
    "    \n",
    "    train_click_i = train_click_list[Ti]     \n",
    "    test_click_i = test_click_list[Ti]\n",
    "    test_qtime_i = test_qtime_list[Ti]\n",
    "    \n",
    "    NOT_item_dict = {}\n",
    "    for i in range(len(test_qtime_i)):\n",
    "        user_id, time = test_qtime_i.iloc[i, :].values\n",
    "        idx = (train_test_click_ALL['user_id'] == int(user_id)) & (train_test_click_ALL['time'] >= time)\n",
    "        NOT_item_dict[user_id] = set(train_test_click_ALL[idx]['item_id'].values)\n",
    "        \n",
    "    TA1, TA2 = 0, Ti\n",
    "    TB1, TB2 = 0, Ti\n",
    "    train_click = pd.concat(train_click_list[TA1: TA2 + 1], axis = 0)\n",
    "    test_click = pd.concat(test_click_list[TB1: TB2 + 1], axis = 0)\n",
    "    train_test_click = train_click.append(test_click)\n",
    "    train_test_click.sort_values(['user_id', 'time'], inplace = True)\n",
    "    train_test_click.drop_duplicates(['user_id', 'item_id'], keep = 'last', inplace = True)\n",
    "    train_test_click.reset_index(drop = True,  inplace = True)\n",
    "    sim_item_corr, user_item = CF_train(Ti, train_test_click)\n",
    "    return sim_item_corr, user_item, train_click_i, test_click_i, test_qtime_i, NOT_item_dict, train_test_click\n",
    "\n",
    "def local_train_constr_RUNXING(Ti, now_phase, N_recall = 50):\n",
    "    train_click_list = []\n",
    "    test_click_list = []\n",
    "    \n",
    "    train_click_i = pd.read_csv(path_input_train + '/' + 'underexpose_train_click-{}.csv'.format(Ti), \n",
    "                                   header = None, names=['user_id', 'item_id', 'time'])\n",
    "    test_click_i = pd.read_csv(path_input_test + '/' + 'underexpose_test_click-{}.csv'.format(Ti), \n",
    "                                   header = None, names=['user_id', 'item_id', 'time'])\n",
    "    all_click = pd.concat([train_click_i, test_click_i], axis = 0).reset_index(drop = True)\n",
    "    all_click = all_click.sort_values(['user_id', 'time']).reset_index(drop = True)\n",
    "    train_answer = all_click.groupby(['user_id']).last().reset_index()\n",
    "    \n",
    "    \n",
    "    for i in range(now_phase + 1):\n",
    "        train_click_i = pd.read_csv(path_input_train + '/' + 'underexpose_train_click-{}.csv'.format(i), \n",
    "                                   header = None, names=['user_id', 'item_id', 'time'])\n",
    "        test_click_i = pd.read_csv(path_input_test + '/' + 'underexpose_test_click-{}.csv'.format(i), \n",
    "                                   header = None, names=['user_id', 'item_id', 'time'])\n",
    "        train_click_i.sort_values(['user_id', 'time'], inplace = True)\n",
    "        test_click_i.sort_values(['user_id', 'time'], inplace = True)\n",
    "        \n",
    "        train_click_i.set_index(['user_id', 'item_id'], inplace = True)\n",
    "        train_click_i = train_click_i[~train_click_i.index.isin(\n",
    "            train_answer.set_index(['user_id', 'item_id']).index)].reset_index()\n",
    "    \n",
    "        test_click_i.set_index(['user_id', 'item_id'], inplace = True)\n",
    "        test_click_i = test_click_i[~test_click_i.index.isin(\n",
    "            train_answer.set_index(['user_id', 'item_id']).index)].reset_index()\n",
    "        \n",
    "        train_click_list.append(train_click_i)\n",
    "        test_click_list.append(test_click_i)\n",
    "        \n",
    "        \n",
    "    train_click_ALL = pd.concat(train_click_list, axis = 0)\n",
    "    test_click_ALL = pd.concat(test_click_list, axis = 0)\n",
    "    train_test_click_ALL = train_click_ALL.append(test_click_ALL)\n",
    "    train_test_click_ALL.sort_values(['user_id', 'time'], inplace = True)\n",
    "    \n",
    "    print(\"before:\", train_test_click_ALL.shape)\n",
    "    train_test_click_ALL.drop_duplicates(['user_id', 'item_id'], keep = 'last', inplace = True)\n",
    "    print(\"before:\", train_test_click_ALL.shape)\n",
    "\n",
    "    train_click_i = train_click_list[Ti]     \n",
    "    test_click_i = test_click_list[Ti]\n",
    "    test_qtime_i = train_answer[['user_id', 'time']]\n",
    "\n",
    "    \n",
    "    NOT_item_dict = {}\n",
    "    for i in range(len(test_qtime_i)):\n",
    "        user_id, time = test_qtime_i.iloc[i, :].values\n",
    "        idx = (train_test_click_ALL['user_id'] == int(user_id)) & (train_test_click_ALL['time'] >= time)\n",
    "        NOT_item_dict[user_id] = set(train_test_click_ALL[idx]['item_id'].values)\n",
    "\n",
    "    TA1, TA2 = 0, Ti\n",
    "    TB1, TB2 = 0, Ti\n",
    "    train_click = pd.concat(train_click_list[TA1: TA2 + 1], axis = 0)\n",
    "    test_click = pd.concat(test_click_list[TB1: TB2 + 1], axis = 0)\n",
    "    train_test_click = train_click.append(test_click)\n",
    "    train_test_click.sort_values(['user_id', 'time'], inplace = True)\n",
    "    train_test_click.drop_duplicates(['user_id', 'item_id'], keep = 'last', inplace = True)\n",
    "    train_test_click.reset_index(drop = True,  inplace = True)\n",
    "    \n",
    "    sim_item_corr, user_item = CF_train(Ti, train_test_click)\n",
    "    return (sim_item_corr, user_item, train_click_i, test_click_i, \n",
    "            test_qtime_i, NOT_item_dict, train_test_click, train_answer)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] re-transform time\n"
     ]
    }
   ],
   "source": [
    "print('[+] re-transform time')\n",
    "import datetime\n",
    "def stamp_to_datetime(x):\n",
    "    return datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "N1 = 53251\n",
    "N2 = 1585067656\n",
    "\n",
    "def feature_engineering(df, clicks, recom, qtime):\n",
    "    feat = df[['user_id', 'item_id']].copy()\n",
    "    tmp = clicks.groupby(['user_id']).agg({\"item_id\":['count', 'nunique'], \"time\":['mean', 'std', 'min', 'max']})\n",
    "    tmp.columns = [\"user_\" + \"_\".join(col) for col in tmp.columns.ravel()]\n",
    "    feat = pd.merge(feat, tmp, on = ['user_id'], how = 'left')\n",
    "    \n",
    "    \n",
    "    ##item based\n",
    "    tmp = clicks.groupby(['item_id']).agg({\"user_id\":['count', 'nunique'], \"time\":['mean', 'std', 'min', 'max']})\n",
    "    tmp.columns = [\"item_\" + \"_\".join(col) for col in tmp.columns.ravel()]\n",
    "    feat = pd.merge(feat, tmp, on = ['item_id'], how = 'left')\n",
    "    \n",
    "    tmp = clicks.sort_values([\"item_id\", 'time']).groupby(['item_id']).last()\n",
    "    tmp.columns = [f'last_{col}' for col in tmp.columns]\n",
    "    feat = pd.merge(feat, tmp, on = ['item_id'], how = 'left')\n",
    "\n",
    "    tmp = clicks.sort_values([\"user_id\", 'time']).groupby(['user_id']).last()\n",
    "    tmp.columns = [f'last_us_{col}' for col in tmp.columns]\n",
    "    feat = pd.merge(feat, tmp, on = ['user_id'], how = 'left')\n",
    "\n",
    "    \n",
    "    tmp = clicks.sort_values([\"user_id\", 'time']).groupby(['user_id']).tail(5)\n",
    "    tmp = tmp.groupby(['user_id']).agg({\"time\":['mean', 'std', 'min', 'max']})\n",
    "    tmp.columns = [\"user_tail5\" + \"_\".join(col) for col in tmp.columns.ravel()]\n",
    "    feat = pd.merge(feat, tmp, on = ['user_id'], how = 'left')\n",
    "\n",
    "    qtmp = qtime.copy()\n",
    "    qtmp['time'] = (qtmp['time'] * N2 + N1).astype(int)\n",
    "    qtmp['date'] = pd.to_datetime(qtmp['time'].apply(stamp_to_datetime))    \n",
    "    qtmp['day'] = qtmp['date'].dt.day\n",
    "    qtmp['hour'] = qtmp['date'].dt.hour    \n",
    "    feat = pd.merge(feat, qtmp[['user_id', 'day', 'hour']], on = ['user_id'], how = 'left')\n",
    "      \n",
    "    feat = pd.merge(feat, qtime, on = ['user_id'], how = 'left')\n",
    "    feat = pd.merge(feat, recom, on = ['user_id', 'item_id'], how = 'left')\n",
    "#     feat['rank_sim'] = recom.groupby(['user_id'])['sim'].rank()\n",
    "    feat['t_d1'] = feat['time'] - feat['last_time']\n",
    "    feat['t_d2'] = feat['time'] - feat['last_us_time']\n",
    "    feat.drop(['user_id', 'item_id'], axis = 1, inplace = True)\n",
    "    \n",
    "    return feat\n",
    "\n",
    "\n",
    "def kfold_lgb(train, test):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    import lightgbm as lgb\n",
    "    import gc\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    params = {\n",
    "            'num_leaves':  31,\n",
    "#           'min_child_weight': 0.034,\n",
    "          'feature_fraction': 0.5,\n",
    "          'bagging_fraction':  0.5,\n",
    "#           'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'random_state': 47\n",
    "         } \n",
    "\n",
    "    import datetime\n",
    "    n_kfolds = 5\n",
    "    feats = [col for col in train.columns if col not in ['label', 'user_id']]\n",
    "    n_train, n_test = train.shape[0], test.shape[0]\n",
    "    oof_train, oof_test= np.zeros((n_train,)), np.zeros((n_test,))\n",
    "    score_list, model_list = [], []\n",
    "    skf = StratifiedKFold(n_splits = n_kfolds, \n",
    "                          shuffle = True, random_state = 777).split(train[feats], train['label'])\n",
    "    for i, (train_idx, valid_idx) in enumerate(skf):\n",
    "        print('############################################################ fold = {} / {}'.format(i + 1, n_kfolds))\n",
    "        print('####### cur time = ' + str(datetime.datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")))\n",
    "        X_train, y_train = train.loc[train_idx, feats], train.loc[train_idx, 'label'] \n",
    "        dtrain = lgb.Dataset(X_train, y_train, free_raw_data = True)\n",
    "        del X_train, y_train; gc.collect();\n",
    "              \n",
    "        X_valid, y_valid = train.loc[valid_idx, feats], train.loc[valid_idx, 'label']\n",
    "        dvalid = lgb.Dataset(X_valid, y_valid, reference = dtrain, free_raw_data = True)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = dtrain,\n",
    "            valid_sets = [dvalid],\n",
    "            num_boost_round = 10000,\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "        )\n",
    "        \n",
    "        oof_train[valid_idx] = model.predict(X_valid)\n",
    "        oof_test += model.predict(test[feats])/n_kfolds\n",
    "        score_list.append(roc_auc_score(y_valid, oof_train[valid_idx]))\n",
    "        print(\"period:\", valid_idx,\", the score is\", roc_auc_score(y_valid, oof_train[valid_idx]))\n",
    "        del X_valid, y_valid, dtrain, dvalid, model; gc.collect();\n",
    "    return oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase 7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35346/35346 [01:43<00:00, 340.49it/s]\n",
      "100%|██████████| 117720/117720 [00:34<00:00, 3399.46it/s]\n",
      "100%|██████████| 1701/1701 [00:01<00:00, 1254.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase series: (294763, 3)\n",
      "phase series: (275093, 3)\n",
      "all click: (1243109, 3)\n",
      "all click: (1218677, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35346/35346 [01:41<00:00, 349.62it/s]\n",
      "100%|██████████| 117716/117716 [00:35<00:00, 3304.91it/s]\n",
      "100%|██████████| 19670/19670 [03:02<00:00, 107.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19670, 51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32747/32747 [00:59<00:00, 547.36it/s]\n",
      "100%|██████████| 105221/105221 [02:08<00:00, 819.65it/s] \n",
      "100%|██████████| 1701/1701 [00:11<00:00, 153.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (2798139, 3)\n",
      "before: (1133251, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32747/32747 [00:58<00:00, 562.39it/s]\n",
      "100%|██████████| 105210/105210 [02:04<00:00, 846.56it/s] \n",
      "100%|██████████| 19670/19670 [01:28<00:00, 221.19it/s]\n",
      "/root/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n",
      "/root/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (3442250, 3)\n",
      "test: (1020600, 2)\n",
      "(3442250, 3)\n",
      "(2690147, 3)\n",
      "(2690147, 3)\n",
      "(2690147, 38)\n",
      "over\n",
      "(900275, 2)\n",
      "(900275, 37)\n",
      "over\n",
      "############################################################ fold = 1 / 5\n",
      "####### cur time = 2020/06/11 17:19:43\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.905662\n",
      "[200]\tvalid_0's auc: 0.908337\n",
      "[300]\tvalid_0's auc: 0.909631\n",
      "[400]\tvalid_0's auc: 0.909954\n",
      "[500]\tvalid_0's auc: 0.910203\n",
      "[600]\tvalid_0's auc: 0.910583\n",
      "[700]\tvalid_0's auc: 0.910249\n",
      "Early stopping, best iteration is:\n",
      "[602]\tvalid_0's auc: 0.910588\n",
      "period: [      8      15      17 ... 2690098 2690115 2690143] , the score is 0.910587916911499\n",
      "############################################################ fold = 2 / 5\n",
      "####### cur time = 2020/06/11 17:20:44\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.899638\n",
      "[200]\tvalid_0's auc: 0.903131\n",
      "[300]\tvalid_0's auc: 0.905304\n",
      "[400]\tvalid_0's auc: 0.906879\n",
      "[500]\tvalid_0's auc: 0.906988\n",
      "Early stopping, best iteration is:\n",
      "[455]\tvalid_0's auc: 0.907071\n",
      "period: [      0       1       2 ... 2690130 2690133 2690146] , the score is 0.907070657292528\n",
      "############################################################ fold = 3 / 5\n",
      "####### cur time = 2020/06/11 17:21:35\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.89767\n",
      "[200]\tvalid_0's auc: 0.900378\n",
      "[300]\tvalid_0's auc: 0.901657\n",
      "[400]\tvalid_0's auc: 0.902136\n",
      "[500]\tvalid_0's auc: 0.902068\n",
      "Early stopping, best iteration is:\n",
      "[431]\tvalid_0's auc: 0.902267\n",
      "period: [      5      11      21 ... 2690137 2690140 2690141] , the score is 0.902266641477842\n",
      "############################################################ fold = 4 / 5\n",
      "####### cur time = 2020/06/11 17:22:23\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.902062\n",
      "[200]\tvalid_0's auc: 0.904006\n",
      "[300]\tvalid_0's auc: 0.905124\n",
      "[400]\tvalid_0's auc: 0.906023\n",
      "[500]\tvalid_0's auc: 0.906231\n",
      "[600]\tvalid_0's auc: 0.906309\n",
      "[700]\tvalid_0's auc: 0.90637\n",
      "[800]\tvalid_0's auc: 0.906437\n",
      "[900]\tvalid_0's auc: 0.906686\n",
      "[1000]\tvalid_0's auc: 0.906868\n",
      "[1100]\tvalid_0's auc: 0.906762\n",
      "Early stopping, best iteration is:\n",
      "[1034]\tvalid_0's auc: 0.906927\n",
      "period: [      6      10      13 ... 2690139 2690144 2690145] , the score is 0.9069272981736524\n",
      "############################################################ fold = 5 / 5\n",
      "####### cur time = 2020/06/11 17:23:58\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.892035\n",
      "[200]\tvalid_0's auc: 0.895459\n",
      "[300]\tvalid_0's auc: 0.897677\n",
      "[400]\tvalid_0's auc: 0.899108\n",
      "[500]\tvalid_0's auc: 0.899565\n",
      "[600]\tvalid_0's auc: 0.899654\n",
      "Early stopping, best iteration is:\n",
      "[567]\tvalid_0's auc: 0.899818\n",
      "period: [      4      14      18 ... 2690121 2690129 2690142] , the score is 0.8998179517692826\n",
      "sorting is over.\n",
      "phase 8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35346/35346 [01:43<00:00, 342.38it/s]\n",
      "100%|██████████| 117720/117720 [00:35<00:00, 3290.27it/s]\n",
      "100%|██████████| 1716/1716 [00:22<00:00, 75.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase series: (288652, 3)\n",
      "phase series: (268916, 3)\n",
      "all click: (1243109, 3)\n",
      "all click: (1218904, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35346/35346 [01:40<00:00, 350.77it/s]\n",
      "100%|██████████| 117715/117715 [00:33<00:00, 3557.09it/s]\n",
      "100%|██████████| 19736/19736 [03:28<00:00, 94.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19736, 51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33955/33955 [01:08<00:00, 496.76it/s]\n",
      "100%|██████████| 111208/111208 [02:26<00:00, 760.51it/s] \n",
      "100%|██████████| 1716/1716 [00:12<00:00, 137.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (2808566, 3)\n",
      "before: (1133185, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33955/33955 [01:07<00:00, 505.07it/s]\n",
      "100%|██████████| 111200/111200 [02:22<00:00, 780.23it/s] \n",
      "100%|██████████| 19736/19736 [01:42<00:00, 193.44it/s]\n",
      "/root/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n",
      "/root/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (3453800, 3)\n",
      "test: (1029600, 2)\n",
      "(3453800, 3)\n",
      "(2692396, 3)\n",
      "(2692396, 3)\n",
      "(2692396, 38)\n",
      "over\n",
      "(907705, 2)\n",
      "(907705, 37)\n",
      "over\n",
      "############################################################ fold = 1 / 5\n",
      "####### cur time = 2020/06/11 17:51:00\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.898851\n",
      "[200]\tvalid_0's auc: 0.901261\n",
      "[300]\tvalid_0's auc: 0.902934\n",
      "[400]\tvalid_0's auc: 0.904063\n",
      "[500]\tvalid_0's auc: 0.904806\n",
      "[600]\tvalid_0's auc: 0.904916\n",
      "[700]\tvalid_0's auc: 0.90516\n",
      "[800]\tvalid_0's auc: 0.905116\n",
      "[900]\tvalid_0's auc: 0.905339\n",
      "[1000]\tvalid_0's auc: 0.905411\n",
      "[1100]\tvalid_0's auc: 0.905423\n",
      "Early stopping, best iteration is:\n",
      "[1065]\tvalid_0's auc: 0.905501\n",
      "period: [      8      15      17 ... 2692374 2692379 2692386] , the score is 0.9055014372541699\n",
      "############################################################ fold = 2 / 5\n",
      "####### cur time = 2020/06/11 17:52:41\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.898071\n",
      "[200]\tvalid_0's auc: 0.902157\n",
      "[300]\tvalid_0's auc: 0.903816\n",
      "[400]\tvalid_0's auc: 0.904604\n",
      "[500]\tvalid_0's auc: 0.904505\n",
      "Early stopping, best iteration is:\n",
      "[435]\tvalid_0's auc: 0.904691\n",
      "period: [      0       1       2 ... 2692373 2692375 2692384] , the score is 0.9046911035787557\n",
      "############################################################ fold = 3 / 5\n",
      "####### cur time = 2020/06/11 17:53:30\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.904879\n",
      "[200]\tvalid_0's auc: 0.908201\n",
      "[300]\tvalid_0's auc: 0.90971\n",
      "[400]\tvalid_0's auc: 0.91113\n",
      "[500]\tvalid_0's auc: 0.911268\n",
      "Early stopping, best iteration is:\n",
      "[475]\tvalid_0's auc: 0.911296\n",
      "period: [      5      11      21 ... 2692390 2692393 2692395] , the score is 0.9112957669991487\n",
      "############################################################ fold = 4 / 5\n",
      "####### cur time = 2020/06/11 17:54:23\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.903827\n",
      "[200]\tvalid_0's auc: 0.9068\n",
      "[300]\tvalid_0's auc: 0.908084\n",
      "[400]\tvalid_0's auc: 0.908873\n",
      "[500]\tvalid_0's auc: 0.909178\n",
      "Early stopping, best iteration is:\n",
      "[477]\tvalid_0's auc: 0.909208\n",
      "period: [      6      10      13 ... 2692388 2692392 2692394] , the score is 0.9092084509742069\n",
      "############################################################ fold = 5 / 5\n",
      "####### cur time = 2020/06/11 17:55:15\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.908114\n",
      "[200]\tvalid_0's auc: 0.910485\n",
      "[300]\tvalid_0's auc: 0.911723\n",
      "[400]\tvalid_0's auc: 0.912504\n",
      "[500]\tvalid_0's auc: 0.912676\n",
      "[600]\tvalid_0's auc: 0.912595\n",
      "Early stopping, best iteration is:\n",
      "[504]\tvalid_0's auc: 0.912788\n",
      "period: [      4      14      18 ... 2692378 2692380 2692391] , the score is 0.9127876417284171\n",
      "sorting is over.\n",
      "phase 9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35346/35346 [01:43<00:00, 340.92it/s]\n",
      "100%|██████████| 117720/117720 [00:36<00:00, 3253.85it/s]\n",
      "100%|██████████| 1651/1651 [00:23<00:00, 70.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase series: (278066, 3)\n",
      "phase series: (258161, 3)\n",
      "all click: (1243109, 3)\n",
      "all click: (1219742, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35346/35346 [01:41<00:00, 349.89it/s]\n",
      "100%|██████████| 117714/117714 [00:34<00:00, 3372.50it/s]\n",
      "100%|██████████| 19905/19905 [03:43<00:00, 88.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19905, 51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35346/35346 [01:19<00:00, 442.54it/s]\n",
      "100%|██████████| 117720/117720 [02:50<00:00, 689.20it/s] \n",
      "100%|██████████| 1651/1651 [00:14<00:00, 115.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (2823149, 3)\n",
      "before: (1133016, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35346/35346 [01:18<00:00, 447.58it/s]\n",
      "100%|██████████| 117714/117714 [02:49<00:00, 695.81it/s] \n",
      "100%|██████████| 19905/19905 [01:51<00:00, 178.88it/s]\n",
      "/root/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n",
      "/root/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (3483375, 3)\n",
      "test: (990600, 2)\n",
      "(3483375, 3)\n",
      "(2688677, 3)\n",
      "(2688677, 3)\n",
      "(2688677, 38)\n",
      "over\n",
      "(869749, 2)\n",
      "(869749, 37)\n",
      "over\n",
      "############################################################ fold = 1 / 5\n",
      "####### cur time = 2020/06/11 18:24:36\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.896027\n",
      "[200]\tvalid_0's auc: 0.899755\n",
      "[300]\tvalid_0's auc: 0.902617\n",
      "[400]\tvalid_0's auc: 0.904\n",
      "[500]\tvalid_0's auc: 0.904396\n",
      "[600]\tvalid_0's auc: 0.90463\n",
      "[700]\tvalid_0's auc: 0.904627\n",
      "Early stopping, best iteration is:\n",
      "[648]\tvalid_0's auc: 0.904791\n",
      "period: [      9      16      18 ... 2688648 2688656 2688665] , the score is 0.9047909946864708\n",
      "############################################################ fold = 2 / 5\n",
      "####### cur time = 2020/06/11 18:25:41\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.891584\n",
      "[200]\tvalid_0's auc: 0.895337\n",
      "[300]\tvalid_0's auc: 0.897617\n",
      "[400]\tvalid_0's auc: 0.899429\n",
      "[500]\tvalid_0's auc: 0.900366\n",
      "[600]\tvalid_0's auc: 0.900549\n",
      "[700]\tvalid_0's auc: 0.900726\n",
      "[800]\tvalid_0's auc: 0.900943\n",
      "Early stopping, best iteration is:\n",
      "[794]\tvalid_0's auc: 0.900975\n",
      "period: [      0       1       2 ... 2688661 2688662 2688667] , the score is 0.9009747182221389\n",
      "############################################################ fold = 3 / 5\n",
      "####### cur time = 2020/06/11 18:26:59\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.907367\n",
      "[200]\tvalid_0's auc: 0.910173\n",
      "[300]\tvalid_0's auc: 0.911747\n",
      "[400]\tvalid_0's auc: 0.912786\n",
      "[500]\tvalid_0's auc: 0.912665\n",
      "Early stopping, best iteration is:\n",
      "[477]\tvalid_0's auc: 0.912802\n",
      "period: [      6      12      22 ... 2688673 2688674 2688676] , the score is 0.912802207228983\n",
      "############################################################ fold = 4 / 5\n",
      "####### cur time = 2020/06/11 18:27:52\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.896092\n",
      "[200]\tvalid_0's auc: 0.898214\n",
      "[300]\tvalid_0's auc: 0.898823\n",
      "[400]\tvalid_0's auc: 0.899105\n",
      "[500]\tvalid_0's auc: 0.899204\n",
      "Early stopping, best iteration is:\n",
      "[476]\tvalid_0's auc: 0.899252\n",
      "period: [      7      11      14 ... 2688666 2688668 2688669] , the score is 0.8992520085427269\n",
      "############################################################ fold = 5 / 5\n",
      "####### cur time = 2020/06/11 18:28:44\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.893109\n",
      "[200]\tvalid_0's auc: 0.896805\n",
      "[300]\tvalid_0's auc: 0.898622\n",
      "[400]\tvalid_0's auc: 0.898814\n",
      "[500]\tvalid_0's auc: 0.898759\n",
      "Early stopping, best iteration is:\n",
      "[426]\tvalid_0's auc: 0.898841\n",
      "period: [      4      15      19 ... 2688659 2688672 2688675] , the score is 0.8988405708540167\n",
      "sorting is over.\n"
     ]
    }
   ],
   "source": [
    "now_phase = 9\n",
    "result = pd.DataFrame()\n",
    "file_save_path = path + '/local_result/'\n",
    "local_pred = pd.DataFrame()\n",
    "for phase in range(7, now_phase + 1):\n",
    "    print(f\"phase {phase}:\")\n",
    "    recall_num = 50\n",
    "    recall_click, test_click_ZW, test_qtime = local_test_constr(phase, now_phase, recall_num)\n",
    "    item_sim_list, user_item_list = get_sim_item(recall_click, 'user_id', 'item_id', use_iif=False)\n",
    "    top50_click, cut_dict = df_constr(recall_click, test_qtime)\n",
    "    test_res_ZW, test_recom_ZW = recommend_pars(top50_click, test_qtime, recall_num)\n",
    "    \n",
    "    recall_num = 50\n",
    "    recall_click, train_click_ZW, train_answer_ZW = local_train_constr(phase, now_phase, recall_num)\n",
    "    train_qtime = train_answer_ZW[['user_id','time']]\n",
    "    item_sim_list, user_item_list = get_sim_item(recall_click, 'user_id', 'item_id', use_iif=False)\n",
    "\n",
    "    top50_click, cut_dict = df_constr(recall_click, train_qtime)\n",
    "    train_res_ZW, train_recom_ZW = recommend_pars(top50_click, train_qtime, recall_num)\n",
    "    print(train_res_ZW.shape)\n",
    "    \n",
    "    \n",
    "    sim_item_corr, user_item, train_click_i, test_click_i, \\\n",
    "    test_qtime_i, NOT_item_dict, test_click_RX = local_test_constr_RUNXING(phase, now_phase)\n",
    "    test_res_RX, test_recom_RX = CF_predict(train_click_i, test_click_i, phase, NOT_item_dict, \n",
    "                                             test_qtime_i, sim_item_corr, user_item, N_recall = 50)\n",
    "\n",
    "\n",
    "    sim_item_corr, user_item, train_click_i, test_click_i, \\\n",
    "    test_qtime_i, NOT_item_dict, train_click_RX, train_answer_RX = local_train_constr_RUNXING(phase, now_phase)\n",
    "    train_res_RX, train_recom_RX = CF_predict(train_click_i, test_click_i, phase, NOT_item_dict, \n",
    "                                             test_qtime_i, sim_item_corr, user_item, N_recall = 25)\n",
    "\n",
    "\n",
    "    \n",
    "    train_rec = pd.read_csv(file_save_path + f'hybrid_emb_update_v3_train_phase{phase}.csv')\n",
    "    test_rec = pd.read_csv(file_save_path + f'hybrid_emb_update_v3_test_phase{phase}.csv')\n",
    "    \n",
    "    test_rec['score'] = \\\n",
    "            (test_rec['sim_all'] + test_rec['sim_txt']) * 0.3 + \\\n",
    "            (test_rec['sim_w2v'] + test_rec['sim_w2v_10w']) * 0.8 + test_rec['sim_w2v_gr'] * 0.1\n",
    "    train_rec['score'] = \\\n",
    "            (train_rec['sim_all'] + train_rec['sim_txt']) * 0.3 + \\\n",
    "            (train_rec['sim_w2v'] + train_rec['sim_w2v_10w']) * 0.8 + train_rec['sim_w2v_gr'] * 0.1\n",
    "    \n",
    "    train_res_new = get_predict(train_rec, 'score', top50_click, 100)\n",
    "    test_res_new = get_predict(test_rec, 'score', top50_click, 500)\n",
    "    train_rec = train_rec.sort_values(['user_id', 'score'], ascending  = False).groupby(['user_id']).head(100)\n",
    "    test_rec = test_rec.sort_values(['user_id', 'score'], ascending  = False).groupby(['user_id']).head(500)\n",
    "#     train_rec.fillna(0, inplace = True)\n",
    "#     test_rec.fillna(0, inplace = True)\n",
    "    \n",
    "\n",
    "    train_click = train_click_ZW\n",
    "    train_answer = train_answer_ZW\n",
    "    test_click = test_click_ZW\n",
    "\n",
    "    train_res = pd.merge(train_res_ZW, train_res_RX, on = ['user_id'], how = 'inner')\n",
    "    train_res = pd.merge(train_res, train_res_new, on = ['user_id'], how = 'inner')\n",
    "    train_recom = pd.merge(train_recom_ZW, train_recom_RX, on = ['user_id', 'item_id'], \n",
    "                            how = 'outer').reset_index(drop = True)\n",
    "    train_recom = pd.merge(train_recom, train_rec, on = ['user_id', 'item_id'], \n",
    "                            how = 'outer').reset_index(drop = True)\n",
    "\n",
    "    test_res = pd.merge(test_res_ZW, test_res_RX, on = ['user_id'], how = 'inner')\n",
    "    test_res = pd.merge(test_res, test_res_new, on = ['user_id'], how = 'inner')\n",
    "\n",
    "    test_recom = pd.merge(test_recom_ZW, test_recom_RX, on = ['user_id', 'item_id'], \n",
    "                                how = 'outer').reset_index(drop = True)\n",
    "    test_recom = pd.merge(test_recom, test_rec, on = ['user_id', 'item_id'], \n",
    "                                how = 'outer').reset_index(drop = True)\n",
    "\n",
    "    test_res.columns = ['user_id'] + [i for i in range(1, test_res.shape[1])]\n",
    " \n",
    "\n",
    "    train = train_res.set_index(['user_id']).unstack().reset_index()\n",
    "    train = train.iloc[:, 1:].astype(int)\n",
    "    train.columns = ['user_id', 'item_id']\n",
    "    train = pd.merge(train, train_answer.reset_index()[['user_id', 'item_id', 'time']],\n",
    "                       on = ['user_id', 'item_id'], how = 'left')\n",
    "    train['time'].fillna(-1, inplace = True)\n",
    "    train['time'] = np.where(train['time'] != -1, 1, 0)\n",
    "    train.columns = ['user_id', 'item_id', 'label']\n",
    "\n",
    "    test = test_res.set_index(\"user_id\").unstack().reset_index()\n",
    "    test = test.iloc[:, 1:].astype(int)\n",
    "    test.columns = ['user_id', 'item_id']\n",
    "\n",
    "    print(\"train:\",train.shape)\n",
    "    print(\"test:\",test.shape)\n",
    "    \n",
    "    print(train.shape)\n",
    "    train = train.drop_duplicates(['user_id', 'item_id']).reset_index(drop = True)\n",
    "    print(train.shape)\n",
    "    test = test.drop_duplicates(['user_id', 'item_id']).reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    train = train[['user_id', 'item_id', 'label']]\n",
    "    print(train.shape)\n",
    "    qtime = train_answer[['user_id', 'time']]\n",
    "    tr_feat = feature_engineering(train, train_click, train_recom, qtime)\n",
    "    train[tr_feat.columns] = tr_feat\n",
    "    print(train.shape)\n",
    "    print(\"over\")\n",
    "\n",
    "    test = test[['user_id', 'item_id']]\n",
    "    print(test.shape)\n",
    "    qtime = pd.read_csv(test_path + '/underexpose_test_qtime-{}.csv'.format(phase), \n",
    "                      header = None, names = ['user_id', 'time'])\n",
    "\n",
    "    ts_feat = feature_engineering(test, test_click, test_recom, qtime)\n",
    "    test[ts_feat.columns] = ts_feat\n",
    "    print(test.shape)\n",
    "    print(\"over\")\n",
    "    \n",
    "    train.drop(columns=[\"rank\"], inplace = True)\n",
    "    test.drop(columns=[\"rank\"], inplace = True)\n",
    "    \n",
    "    \n",
    "    preds = kfold_lgb(train, test)\n",
    "    test['pred'] = preds\n",
    "    local_pred = local_pred.append(test[['user_id', 'item_id', 'pred']])\n",
    "    sort_res = test.sort_values(['user_id', 'pred'], ascending = False).groupby('user_id')['item_id'].apply(\n",
    "    lambda x: ','.join([str(i) for i in x])).str.split(',', expand=True).reset_index()\n",
    "    sort_res = sort_res.iloc[:, :51]\n",
    "    test.drop([\"pred\"], axis = 1, inplace = True)\n",
    "    \n",
    "    result = result.append(sort_res)\n",
    "    print(\"sorting is over.\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local_pred = local_pred.sort_values(['user_id', 'pred'], ascending = False).reset_index(drop = True)\n",
    "local_pred.to_csv(file_save_path + \"/local_200611_final_result.csv\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "phase 7\n",
      "update score: 0.11508925051116507\n",
      "update recall: 0.2551440329218107\n",
      "phase 8\n",
      "update score: 0.10408997307205409\n",
      "update recall: 0.23076923076923078\n",
      "phase 9\n",
      "update score: 0.09244287475767646\n",
      "update recall: 0.2113870381586917\n",
      "final update score: 0.3116220983408956\n"
     ]
    }
   ],
   "source": [
    "print(\"*\" * 50)\n",
    "score = 0\n",
    "for phase in range(7, now_phase + 1):\n",
    "    print(\"phase\", phase)\n",
    "    ans_tmp = pd.read_csv(test_path + '/underexpose_test_label-{}.csv'.format(phase), \n",
    "                      header = None, names = ['user_id', 'item_id', 'time'])\n",
    "    tmp = pd.merge(ans_tmp, result, on = ['user_id'], how = 'inner')\n",
    "    print(\"update score:\", ndcg_metric(tmp))\n",
    "    recall_num = 0\n",
    "    for rank in range(50):\n",
    "        recall_num += np.sum(tmp[rank].astype(int) == tmp['item_id'])\n",
    "    print(\"update recall:\",recall_num/(tmp.shape[0]))\n",
    "    score += ndcg_metric(tmp)\n",
    "    \n",
    "print(\"final update score:\", score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "phase 7\n",
      "[+] NDCG@FULL = 0.115089, NDCG@HALF = 0.103834\n",
      "[+]  HIT@FULL = 0.255144,  HIT@HALF = 0.235103\n",
      "phase 8\n",
      "[+] NDCG@FULL = 0.104090, NDCG@HALF = 0.100652\n",
      "[+]  HIT@FULL = 0.230769,  HIT@HALF = 0.218653\n",
      "phase 9\n",
      "[+] NDCG@FULL = 0.092443, NDCG@HALF = 0.073283\n",
      "[+]  HIT@FULL = 0.211387,  HIT@HALF = 0.177313\n",
      "half score: 0.2777687981724739\n"
     ]
    }
   ],
   "source": [
    "print(\"*\" * 50)\n",
    "score = 0\n",
    "for phase in range(7, now_phase + 1):\n",
    "    print(\"phase\", phase)\n",
    "    ans_tmp = pd.read_csv(test_path + '/underexpose_test_label-{}.csv'.format(phase), \n",
    "                      header = None, names = ['user_id', 'item_id', 'time'])\n",
    "    tmp = pd.merge(ans_tmp[['user_id']], result, on = ['user_id'], how = 'inner')\n",
    "    click_train = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(phase), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "    click_test = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(phase), header=None,  names=['user_id', 'item_id', 'time'])\n",
    "    all_click = click_train.append(click_test).reset_index(drop  = True)\n",
    "    \n",
    "    LB_CF = get_score_LB(ans_tmp, tmp.astype(int), click_train.head(0), all_click )\n",
    "    score += LB_CF[1]\n",
    "    \n",
    "print(\"half score:\", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
